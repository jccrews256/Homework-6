[
  {
    "objectID": "Homework 6.html",
    "href": "Homework 6.html",
    "title": "Homework 6",
    "section": "",
    "text": "Before we get started, we need to read in the core tidyverse packages.\n\nlibrary(tidyverse)\n\nWarning: package 'tidyverse' was built under R version 4.3.3\n\n\nWarning: package 'tidyr' was built under R version 4.3.3\n\n\nWarning: package 'purrr' was built under R version 4.3.3\n\n\nWarning: package 'dplyr' was built under R version 4.3.3\n\n\nWarning: package 'forcats' was built under R version 4.3.3\n\n\nWarning: package 'lubridate' was built under R version 4.3.3\n\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.4.4     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\n\nTask 1: Conceptual Questions\n\nWhat is the purpose of the lapply() function? What is the equivalent purrr function?\n\n\nThe purpose of lapply() is to easily apply a function to each element of a list. The equivalent purrr function is map().\n\n\nSuppose we have a list called my_list. Each element of the list is a numeric data frame (all columns are numeric). We want use lapply() to run the code cor(numeric_matrix, method = \"kendall\") on each element of the list. Write code to do this below!\n\n\nlapply(X=my_list,FUN=cor,method=\"kendall\")\n\n\nWhat are two advantages of using purrr functions instead of the BaseR apply family?\n\n\nThere are several associated helper functions and some convenient shorthand functionality.\n\n\nThere is more consistency across purrr functions in terms of argument naming schemes, etc.\n\n\nWhat is a side-effect function?\n\n\nA side-effect function is a function where the main purpose is not to modify objects in the environment, but rather to output other things like data files, text, and plots. Examples are plot() and write_csv.\n\n\nWhy can you name a variable sd in a function and not cause any issues with the sd function?\n\n\nBecause R creates temporary function environment while the function is running, preventing any action within the function environment from overwriting anything outside that environment. The exception is, of course, the object assigned the function output.\n\n\n\nTask 2: Writing R Functions\nLet’s build some functions to calculate standard measures of prediction accuracy!\n\n1.\nTo start, we will create the function getRMSE() to calculate the root mean square error (RMSE), or the square root of the average squared difference between a model’s predictions and the actual values of the response variable.\n\n#Writing a function to calculate root mean square error for model predictions\ngetRMSE&lt;-function(resp,pred,...) {\n  #Checking validity of pred and resp inputs\n  if (!is.atomic(pred) | !is.numeric(pred)) stop(\"pred must be a numeric vector\")\n  if (!is.atomic(resp) | !is.numeric(resp)) stop(\"resp must be a numeric vector\") \n  \n  #Calculating the squared differences between predictions and true values\n  squared_diffs&lt;-(resp-pred)^2\n  \n  RMSE&lt;-sqrt(mean(squared_diffs,...))\n  \n  return(list(\"RMSE\"=RMSE))\n}\n\n\n\n2.\nNow let’s test our function by running a regression on some similated data and passing the response values and predictions to getRMSE(). We will start by generating our data.\n\n#Setting seed for reproducibility\nset.seed(10)\n\n#Generating 100 simulated x and resp values\nn &lt;- 100\nx &lt;- runif(n)\nresp &lt;- 3 + 10 * x + rnorm(n)\n\n#Generating predictions from regressing resp on x\npred &lt;- predict(lm(resp ~ x), data.frame(x))\n\nNow that we have our data, let’s apply our getRMSE() function.\n\n#Generating RMSE for simulated data\ngetRMSE(resp=resp,pred=pred)\n\n$RMSE\n[1] 0.9581677\n\n\nAs a final check, we will see how our function handles missing values when we include na.rm=TRUE and when we do not.\n\n#Converting the 12th and 48th response values to missing\nresp_nas&lt;-resp\nresp_nas[c(12,48)]&lt;-NA_real_\n\n#Testing getRMSE function with na.rm=TRUE\ngetRMSE(resp=resp_nas,pred=pred,na.rm=TRUE)\n\n$RMSE\n[1] 0.9626301\n\n#Testing function without na.rm=TRUE\ngetRMSE(resp=resp_nas,pred=pred)\n\n$RMSE\n[1] NA\n\n\nInteresting! Note that our function overcomes the missing values and works as intended when we specify na.rm=TRUE; it calculates the RMSE for the observations where the response is not missing. However, when we don’t include this specification, our function returns NA.\n\n\n3.\nNow we will create the getMAE() function to calculate the mean absolute error (MAE), or the average absolute difference between response values and their predictions.\n\n#Writing a function to calculate mean absolute error for model predictions\ngetMAE&lt;-function(resp,pred,...) {\n  #Checking validity of pred and resp inputs\n  if (!is.atomic(pred) | !is.numeric(pred)) stop(\"pred must be a numeric vector\")\n  if (!is.atomic(resp) | !is.numeric(resp)) stop(\"resp must be a numeric vector\") \n  \n  #Calculating the absolute differences between predictions and true values\n  abs_diffs&lt;-abs(resp-pred)\n  \n  MAE&lt;-mean(abs_diffs,...)\n  \n  return(list(\"MAE\"=MAE))\n}\n\n\n\n4.\nLet’s use our simulated data to test the getMAE() function.\n\n#Generating MAE for simulated data\ngetMAE(resp=resp,pred=pred)\n\n$MAE\n[1] 0.8155776\n\n\nLet’s see what happens when our data include NAs. As we did for problem 2, we will see what happens when we do and do not specify na.rm=TRUE.\n\n#Testing getMAE function with na.rm=TRUE\ngetMAE(resp=resp_nas,pred=pred,na.rm=TRUE)\n\n$MAE\n[1] 0.8180911\n\n#Testing function without na.rm=TRUE\ngetMAE(resp=resp_nas,pred=pred)\n\n$MAE\n[1] NA\n\n\nAs was the case for getRMSE(), when we include na.rm=TRUE, the function works as intended and calculates the MAE for the observations where the response is not missing. Meanwhile, when we do not include this additional specification, our function cannot overcome the missing values and returns NA.\n\n\n5.\nLet’s build a wrapper function that calls either or both of our functions above. We will call this function get_pred_stat. By default, the function will return both the RMSE and MAE.\nNote to grader: I include data validity checks in the individual getRMSE and getMAE functions, so it is not necessary to add those checks into the explicit code for the wrapper function.\n\n#Creating a wrapper function for getRMSE and getMAE\nget_pred_stats&lt;-function(resp,pred,stats=c(\"RMSE\",\"MAE\"),...) {\n  #Initially creating a list to return\n  return_list&lt;-list()\n  \n  #Running getRMSE if RMSE is requested\n  if (\"RMSE\" %in% stats) {\n    return_list$RMSE&lt;-unlist(getRMSE(resp=resp,pred=pred,...))\n  }\n  \n  #Running getMAE if MAE is requested\n  if (\"MAE\" %in% stats) {\n    return_list$MAE&lt;-unlist(getMAE(resp=resp,pred=pred,...))\n  }\n  \n  #Returning requested model prediction stats\n  return(return_list)\n}\n\nTo test this function, let’s first request the RMSE for our response and prediction values created in problem 2.\n\n#Requesting RMSE using get_pred_stats\nget_pred_stats(resp=resp,pred=pred,stats=\"RMSE\")\n\n$RMSE\n     RMSE \n0.9581677 \n\n\nGreat, the results are the same as in problem 2!\nNext, let’s request the MAE for the same response and prediction values.\n\n#Requesting MAE using get_pred_stats\nget_pred_stats(resp=resp,pred=pred,stats=\"MAE\")\n\n$MAE\n      MAE \n0.8155776 \n\n\nWe got exactly what we wanted, as the results were the same as in problem 4.\nLet’s confirm that when we don’t specify a stat, we receive both RMSE and MAE.\n\n#Testing function using default stats (both RMSE and MAE)\nget_pred_stats(resp=resp,pred=pred)\n\n$RMSE\n     RMSE \n0.9581677 \n\n$MAE\n      MAE \n0.8155776 \n\n\nExcellent, the function does indeed return both statistics.\nNow, let’s see what happens when we use the modified response vector that includes two Nas. As we did above, we will separately run the function with and without na.rm=TRUE specified.\n\n#Testing NA handling when na.rm=TRUE specified\nget_pred_stats(resp=resp_nas,pred=pred,na.rm=TRUE)\n\n$RMSE\n     RMSE \n0.9626301 \n\n$MAE\n      MAE \n0.8180911 \n\n#Testing NA handling when na.rm=TRUE not specified\nget_pred_stats(resp=resp_nas,pred=pred)\n\n$RMSE\nRMSE \n  NA \n\n$MAE\nMAE \n NA \n\n\nAs was the case for our individual getRMSE and getMAE functions when we specify na.rm=TRUE, the function excludes the observations with missing values and calculates the RMSE and MAE. When we do not specify na.rm=TRUE, NAs are returned, which matches the behavior of the individual functions.\nAs a final set of checks, we will test the function when incorrect data are specified. First, let’s feed a character vector for pred.\n\n#Testing the function with a character vector passed for pred\nget_pred_stats(resp=resp,pred=as.character(pred))\n\nError in getRMSE(resp = resp, pred = pred, ...): pred must be a numeric vector\n\n\nThis is exactly what we wanted to happen; the function returns an error and informs us that pred must be a numeric vector.\nNow, let’s feed a data frame for resp and see what the function returns.\n\n#Testing the function with a data frame passed for resp\nget_pred_stats(resp=data.frame(resp,resp),pred=pred)\n\nError in getRMSE(resp = resp, pred = pred, ...): resp must be a numeric vector\n\n\nAgain, the function performed as we wanted it to, indicating that resp must also be a numeric vector.\n\n\n\nTask 3: Practice with purrr\nFor the third task, let’s explore some purrr functions.\n\n1.\nTo begin, we will compare the following methods for extracting coefficients from a regression output:\n\nThe $ operator\ncoef()\npluck() from purrr\n\n\n#Capturing an example regression fit\nlm_fit1 &lt;- lm(Sepal.Length ~ Sepal.Width + Species, data = iris)\n\n#Using $ to extract coefficients\nlm_fit1$coefficients\n\n      (Intercept)       Sepal.Width Speciesversicolor  Speciesvirginica \n        2.2513932         0.8035609         1.4587431         1.9468166 \n\n#Using coef to extract coefficients\ncoef(lm_fit1)\n\n      (Intercept)       Sepal.Width Speciesversicolor  Speciesvirginica \n        2.2513932         0.8035609         1.4587431         1.9468166 \n\n#Using pluck to extract coefficients\nlm_fit1 |&gt; pluck(coefficients)\n\n      (Intercept)       Sepal.Width Speciesversicolor  Speciesvirginica \n        2.2513932         0.8035609         1.4587431         1.9468166 \n\n\nThe results are identical; each method returns the same named vector of coefficients.\n\n\n2.\nLet’s extend the use of pluck() across multiple regression fits to really see what purrr can do. To do this, we will fit three additional models and combine the four regression outputs in a single list.\n\n#Fitting three additional example models\nlm_fit2 &lt;- lm(Sepal.Length ~ Sepal.Width, data = iris)\nlm_fit3 &lt;- lm(Sepal.Length ~ Petal.Width + Sepal.Width + Species, data = iris)\nlm_fit4 &lt;- lm(Sepal.Length ~ Petal.Width + Petal.Length + Sepal.Width + Species,\ndata = iris)\n\n#Combining regression outputs in a single list\nfits &lt;- list(lm_fit1, lm_fit2, lm_fit3, lm_fit4)\n\n#Extracting the coefficients for each model fit\nfits |&gt; \n  map(pluck,coefficients)\n\n[[1]]\n      (Intercept)       Sepal.Width Speciesversicolor  Speciesvirginica \n        2.2513932         0.8035609         1.4587431         1.9468166 \n\n[[2]]\n(Intercept) Sepal.Width \n  6.5262226  -0.2233611 \n\n[[3]]\n      (Intercept)       Petal.Width       Sepal.Width Speciesversicolor \n        2.5210733         0.3715768         0.6982260         0.9881297 \n Speciesvirginica \n        1.2375878 \n\n[[4]]\n      (Intercept)       Petal.Width      Petal.Length       Sepal.Width \n        2.1712663        -0.3151552         0.8292439         0.4958889 \nSpeciesversicolor  Speciesvirginica \n       -0.7235620        -1.0234978 \n\n\nThat was easy enough, and we can now compare coefficients across the four models.\n\n\n3.\nIt could be useful to have confidence intervals for each coefficient above, so let’s use map() in combination with confint() to obtain those.\n\n#Extracting confidence intervals for the coefficients from each model fit\nfits |&gt;\n  map(confint)\n\n[[1]]\n                      2.5 %   97.5 %\n(Intercept)       1.5206309 2.982156\nSepal.Width       0.5933983 1.013723\nSpeciesversicolor 1.2371791 1.680307\nSpeciesvirginica  1.7491525 2.144481\n\n[[2]]\n                2.5 %     97.5 %\n(Intercept)  5.579865 7.47258038\nSepal.Width -0.529820 0.08309785\n\n[[3]]\n                        2.5 %    97.5 %\n(Intercept)        1.74261803 3.2995285\nPetal.Width       -0.02042746 0.7635811\nSepal.Width        0.46205710 0.9343950\nSpeciesversicolor  0.44520784 1.5310516\nSpeciesvirginica   0.46412393 2.0110518\n\n[[4]]\n                       2.5 %      97.5 %\n(Intercept)        1.6182321  2.72430044\nPetal.Width       -0.6140049 -0.01630542\nPetal.Length       0.6937939  0.96469395\nSepal.Width        0.3257653  0.66601260\nSpeciesversicolor -1.1982739 -0.24885002\nSpeciesvirginica  -1.6831329 -0.36386273\n\n\nAgain, that was incredibly easy, and now we can identify the coefficients in each model that are statistically different from 0.\n\n\n4.\nLet’s plot the residuals for each fit to visualize in-sample prediction error. To start, we will utilize the map function to capture the residuals for each fit and the walk function to generate a histogram for each fit.\n\n#Specifying a 2x2 grid of histograms\npar(mfrow = c(2, 2))\n\n#Extracting residuals and creating histograms\nfits |&gt;\n  map(resid) |&gt;\n  walk(hist,xlab=\"Residual\")\n\n\n\n\n\n\n\n\nThat wasn’t too hard, but the titles are not informative.\n\n\n5.\nLet’s give names to the elements of the residuals list to see if that fixes the issue.\n\n#Specifying a 2x2 grid of histograms\npar(mfrow = c(2, 2))\n\n#Extracting residuals, naming list elements, and creating histograms\nfits |&gt;\n  map(resid) |&gt;\n  set_names(c(\"fit1\",\"fit2\",\"fit3\",\"fit4\")) |&gt;\n  walk(hist,xlab=\"Residual\")\n\n\n\n\n\n\n\n\nNo luck, as there is no clear way to pass the names to the hist function.\nIt seems we need to use the walk variant iwalk to gain access to the element names. Let’s combine iwalk with an anonymous function that indicates we want to run the hist function for each set of residuals and specify a title that incorporates the element names.\n\n#Specifying a 2x2 grid of histograms\npar(mfrow = c(2, 2))\n\n#Extracting residuals, naming list elements, and creating histograms with names incorporated in titles\nfits |&gt;\n  map(resid) |&gt;\n  set_names(c(\"fit1\",\"fit2\",\"fit3\",\"fit4\")) |&gt;\n  iwalk(\\(x,idx) hist(x,main=paste0(\"Histogram of Residuals for \",idx),xlab=\"Residual\"))\n\n\n\n\n\n\n\n\nIt worked! Now we know which histogram corresponds to which fit. The distribution of residuals is generally symmetric, which is what we want to see. There is visual evidence of a slight right-skew for the second model, although that is not the case when we add regressors to the model (fit2 only includes Sepal.Width as a regressor). Additionally, this slight asymmetry could be the product of the current bin width and positions rather than actual asymmetry."
  }
]